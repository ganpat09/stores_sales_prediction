{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from saleStorePredictor import logging\n",
    "from saleStorePredictor.entity.config_entity import ModelEvaluationConfig\n",
    "from saleStorePredictor.entity.artifact_entity import DataIngestionArtifact,ModelTrainerArtifact,ModelEvaluationArtifact\n",
    "from saleStorePredictor.constants import *\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from saleStorePredictor.utils import save_json,load_json, read_yaml,load_bin,write_yaml\n",
    "from saleStorePredictor.entity.model_factory import evaluate_regression_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "\n",
    "    def __init__(self, model_evaluation_config: ModelEvaluationConfig,\n",
    "                 data_ingestion_artifact: DataIngestionArtifact,\n",
    "                 model_trainer_artifact: ModelTrainerArtifact):\n",
    "        try:\n",
    "          #  logging.info(f\"{'>>' * 30}Model Evaluation log started.{'<<' * 30} \")\n",
    "            self.model_evaluation_config = model_evaluation_config\n",
    "            self.model_trainer_artifact = model_trainer_artifact\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "           \n",
    "        except Exception as e:\n",
    "            raise  e\n",
    "\n",
    "    def get_best_model(self):\n",
    "        try:\n",
    "            model = None\n",
    "            model_evaluation_file_path = self.model_evaluation_config.model_evaluation_file_path\n",
    "\n",
    "            if not os.path.exists(model_evaluation_file_path):\n",
    "               # logging.info(f\"{'>>' * 30}Model evaluation file not found.<<<<<<<\")\n",
    "                write_yaml(model_evaluation_file_path)\n",
    "                return model\n",
    "\n",
    "         #   logging.info(f\"{'>>' * 30}Model Evaluation logging started  {model_evaluation_file_path}.<<<<<<<< <<\")    \n",
    "            model_eval_file_content = read_yaml(Path(model_evaluation_file_path))\n",
    "\n",
    "            model_eval_file_content = dict() if model_eval_file_content is None else model_eval_file_content\n",
    "\n",
    "            if BEST_MODEL_KEY not in model_eval_file_content:\n",
    "                return model\n",
    "\n",
    "            logging.info(f\"{model_eval_file_content[BEST_MODEL_KEY][MODEL_PATH_KEY]}\")    \n",
    "\n",
    "            model = load_bin(model_eval_file_content[BEST_MODEL_KEY][MODEL_PATH_KEY])\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            raise  e\n",
    "\n",
    "\n",
    "    def update_evaluation_report(self, model_evaluation_artifact: ModelEvaluationArtifact):\n",
    "        try:\n",
    "            eval_file_path = Path(self.model_evaluation_config.model_evaluation_file_path)\n",
    "            model_eval_content = read_yaml(eval_file_path).to_dict()\n",
    "            model_eval_content = dict() if model_eval_content is None else model_eval_content\n",
    "            \n",
    "            \n",
    "            previous_best_model = None\n",
    "            if BEST_MODEL_KEY in model_eval_content:\n",
    "                previous_best_model = model_eval_content[BEST_MODEL_KEY]\n",
    "\n",
    "            logging.info(f\"Previous eval result: {model_eval_content}\")\n",
    "            eval_result = {\n",
    "                BEST_MODEL_KEY: {\n",
    "                    MODEL_PATH_KEY: str(model_evaluation_artifact.evaluated_model_path),\n",
    "                }\n",
    "            }\n",
    "\n",
    "            if previous_best_model is not None:\n",
    "                model_history = {self.model_evaluation_config.time_stamp: previous_best_model}\n",
    "                if HISTORY_KEY not in model_eval_content:\n",
    "                    history = {HISTORY_KEY: model_history}\n",
    "                    eval_result.update(history)\n",
    "                else:\n",
    "                    model_eval_content[HISTORY_KEY].update(model_history)\n",
    "\n",
    "            model_eval_content.update(eval_result)\n",
    "            logging.info(f\"Updated eval result:{model_eval_content}\")\n",
    "            logging.info(f\"type eval result:{(model_eval_content)}\")\n",
    "\n",
    "            write_yaml(eval_file_path, model_eval_content)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def initiate_model_evaluation(self) -> ModelEvaluationArtifact:\n",
    "        try:\n",
    "            trained_model_file_path = Path(self.model_trainer_artifact.trained_model_file_path)\n",
    "            trained_model_object = load_bin(trained_model_file_path)\n",
    "\n",
    "            train_file_path = Path(self.data_ingestion_artifact.train_file_path)\n",
    "            test_file_path =   Path(self.data_ingestion_artifact.test_file_path)\n",
    "\n",
    "            schema_file_path =  SCHEMA_FILE_PATH\n",
    "\n",
    "\n",
    "           # train_dataframe = load_data(file_path=train_file_path,schema_file_path=schema_file_path, )\n",
    "            #test_dataframe = load_data(file_path=test_file_path,schema_file_path=schema_file_path,)\n",
    "\n",
    "\n",
    "\n",
    "           # logging.info(f\"{train_file_path} {schema_file_path}\")\n",
    "           # logging.info(f\"{test_file_path}\")\n",
    "            train_dataframe = pd.read_csv(train_file_path)\n",
    "            test_dataframe = pd.read_csv(test_file_path)\n",
    "\n",
    "           \n",
    "                                                          \n",
    "            schema_content = read_yaml(schema_file_path)\n",
    "            target_column_name = schema_content.target_column\n",
    "\n",
    "           # logging.info(f\"{target_column_name}\")\n",
    "\n",
    "            # target_column\n",
    "            logging.info(f\"Converting target column into numpy array.\")\n",
    "            train_target_arr = np.array(train_dataframe[target_column_name])\n",
    "            test_target_arr = np.array(test_dataframe[target_column_name])\n",
    "            logging.info(f\"Conversion completed target column into numpy array.\")\n",
    "\n",
    "            # dropping target column from the dataframe\n",
    "            logging.info(f\"Dropping target column from the dataframe.\")\n",
    "            train_dataframe.drop(target_column_name, axis=1, inplace=True)\n",
    "            test_dataframe.drop(target_column_name, axis=1, inplace=True)\n",
    "            logging.info(f\"Dropping target column from the dataframe completed.\")\n",
    "\n",
    "            model = self.get_best_model()\n",
    "\n",
    "            if model is None:\n",
    "                logging.info(\"Not found any existing model. Hence accepting trained model\")\n",
    "                model_evaluation_artifact = ModelEvaluationArtifact(evaluated_model_path=trained_model_file_path,\n",
    "                                                                    is_model_accepted=True)\n",
    "                self.update_evaluation_report(model_evaluation_artifact)\n",
    "                logging.info(f\"Model accepted. Model eval artifact {model_evaluation_artifact} created\")\n",
    "                return model_evaluation_artifact\n",
    "\n",
    "            model_list = [model, trained_model_object]\n",
    "\n",
    "            metric_info_artifact = evaluate_regression_model(model_list=model_list,\n",
    "                                                               X_train=train_dataframe,\n",
    "                                                               y_train=train_target_arr,\n",
    "                                                               X_test=test_dataframe,\n",
    "                                                               y_test=test_target_arr,\n",
    "                                                               base_accuracy=self.model_trainer_artifact.model_accuracy,\n",
    "                                                               )\n",
    "            logging.info(f\"Model evaluation completed. model metric artifact: {metric_info_artifact}\")\n",
    "\n",
    "            if metric_info_artifact is None:\n",
    "                response = ModelEvaluationArtifact(is_model_accepted=False,\n",
    "                                                   evaluated_model_path=trained_model_file_path\n",
    "                                                   )\n",
    "                logging.info(response)\n",
    "                return response\n",
    "\n",
    "            if metric_info_artifact.index_number == 1:\n",
    "                model_evaluation_artifact = ModelEvaluationArtifact(evaluated_model_path=trained_model_file_path,\n",
    "                                                                    is_model_accepted=True)\n",
    "                self.update_evaluation_report(model_evaluation_artifact)\n",
    "                logging.info(f\"Model accepted. Model eval artifact {model_evaluation_artifact} created\")\n",
    "\n",
    "            else:\n",
    "                logging.info(\"Trained model is no better than existing model hence not accepting trained model\")\n",
    "                model_evaluation_artifact = ModelEvaluationArtifact(evaluated_model_path=trained_model_file_path,\n",
    "                                                                    is_model_accepted=False)\n",
    "            return model_evaluation_artifact\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def __del__(self):\n",
    "        logging.info(f\"{'=' * 20}Model Evaluation log completed.{'=' * 20} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-09-28 10:32:22,701: INFO: common]: yaml file: configs\\config.yaml loaded successfully\n",
      "[2022-09-28 10:32:22,707: INFO: common]: yaml file: params.yaml loaded successfully\n",
      "[2022-09-28 10:32:22,707: INFO: common]: created directory at: artifacts\n",
      "[2022-09-28 10:32:22,707: INFO: configuration]: Model evaluation config: {'model_evaluation_file_name': 'artifacts/model_evaluation/model_evaluation.yaml', 'best_model': 'best_model'}\n",
      "[2022-09-28 10:32:22,707: INFO: common]: created directory at: artifacts/model_evaluation\n",
      "[2022-09-28 10:32:22,707: INFO: common]: json file loaded succesfully from: artifacts\\trained_model\\model_report.json\n",
      "[2022-09-28 10:32:22,921: INFO: common]: binary file loaded from: artifacts\\trained_model\\model.pkl\n",
      "[2022-09-28 10:32:22,945: INFO: common]: yaml file: configs\\schema.yaml loaded successfully\n",
      "[2022-09-28 10:32:22,945: INFO: 3207584564]: Converting target column into numpy array.\n",
      "[2022-09-28 10:32:22,945: INFO: 3207584564]: Conversion completed target column into numpy array.\n",
      "[2022-09-28 10:32:22,945: INFO: 3207584564]: Dropping target column from the dataframe.\n",
      "[2022-09-28 10:32:22,953: INFO: 3207584564]: Dropping target column from the dataframe completed.\n",
      "[2022-09-28 10:32:22,953: INFO: common]: yaml file: artifacts\\model_evaluation\\model_evaluation.yaml loaded successfully\n",
      "[2022-09-28 10:32:22,953: INFO: 3207584564]: Not found any existing model. Hence accepting trained model\n",
      "[2022-09-28 10:32:22,953: INFO: common]: yaml file: artifacts\\model_evaluation\\model_evaluation.yaml loaded successfully\n",
      "[2022-09-28 10:32:22,953: INFO: 3207584564]: Previous eval result: {}\n",
      "[2022-09-28 10:32:22,953: INFO: 3207584564]: Updated eval result:{'best_model': {'model_path': 'artifacts\\\\trained_model\\\\model.pkl'}}\n",
      "[2022-09-28 10:32:22,953: INFO: 3207584564]: type eval result:{'best_model': {'model_path': 'artifacts\\\\trained_model\\\\model.pkl'}}\n",
      "[2022-09-28 10:32:22,961: INFO: common]: yaml file: artifacts\\model_evaluation\\model_evaluation.yaml write successfully\n",
      "[2022-09-28 10:32:22,961: INFO: 3207584564]: Model accepted. Model eval artifact ModelEvaluationArtifact(is_model_accepted=True, evaluated_model_path=WindowsPath('artifacts/trained_model/model.pkl')) created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\FSDS\\DS internship\\stores_sales_prediction\\env\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelEvaluationArtifact(is_model_accepted=True, evaluated_model_path=WindowsPath('artifacts/trained_model/model.pkl'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from saleStorePredictor.config import ConfigurationManager\n",
    "\n",
    "config = ConfigurationManager()\n",
    "\n",
    "model_evaluations = ModelEvaluation(\n",
    "    model_evaluation_config=config.get_model_evaluation_config(),\n",
    "    data_ingestion_artifact=config.get_data_ingestion_artifact(),\n",
    "    model_trainer_artifact=config.get_model_trainer_artifact()\n",
    ")\n",
    "\n",
    "model_evaluations.initiate_model_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cbb0b948f33c3107f482a8001e5ae7cac736ef069ea6ca766c7281b6fed019f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
